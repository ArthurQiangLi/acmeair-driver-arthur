#!/usr/bin/env python

import sys, os
import time
from datetime import datetime, timedelta
import pandas as pd
import matplotlib.pyplot as plt
from sdcclient import SdMonitorClient, IbmAuthHelper

# Configuration values
GUID = '3fed93bc-00f4-4651-8ce2-e73ba4b9a918'
APIKEY = 'mMdgK_7zUpsjHuRmBuQaut8GqecNMqw76XpWMRoQDSxK'
URL = 'https://ca-tor.monitoring.cloud.ibm.com'

# Setup the Sysdig monitoring client
ibm_headers = IbmAuthHelper.get_headers(URL, APIKEY, GUID)
sdclient = SdMonitorClient(sdc_url=URL, custom_headers=ibm_headers)

# Define metrics to monitor
metrics = [
    # {"id": "net.http.error.count", "aggregations": {"time": "avg", "group": "avg"}},
    # {"id": "memory.limit.used.percent", "aggregations": {"time": "avg", "group": "avg"}},
    # {"id": "net.http.ttfb.avg", "aggregations": {"time": "avg", "group": "avg"}}, 
    # {"id": "net.http.requests.count", "aggregations": {"time": "rate", "group": "avg"}}, 
    {"id": "cpu.quota.used.percent", "aggregations": {"time": "avg", "group": "avg"}},
    {"id": "cpu.used.percent", "aggregations": {"time": "avg", "group": "avg"}},
    {"id": "net.request.time", "aggregations": {"time": "max", "group": "avg"}},
    {"id": "net.http.request.time", "aggregations": {"time": "avg", "group": "avg"}}, 
    {"id": "net.http.request.time.worst", "aggregations": {"time": "avg", "group": "max"}}, 
]

# METRICS = [ 
#            {"id": "http.error.count", "aggregations": {"time": "sum", "group": "sum"}}, 
#            {"id": "container.memory.usage.percent", "aggregations": {"time": "avg", "group": "avg"}} ]

# Namespace filter
filter = "kubernetes.namespace.name = 'group-7'"

############################### Parameters #################################
sampling = 10  # Sampling time in seconds
collection_duration = 60*6  # in seconds, from now to N seconds later


progress_length = 60  # Length of the progress bar
output_dir = "./monitor_collections/" # Specify the output directory for saving plots
############################# Initiating data ##############################
# Define the time window for fetching data
start = -sampling  # Fetch data for the last 'sampling' seconds
end = 0

data_list = []# Initialize a list to store collected data
collection_end_time = time.time() + collection_duration# Define data collection duration
############################### Functions ##################################

def display_progress_bar():
    elapsed_time = int(time.time() - (collection_end_time - collection_duration))
    remaining_time = collection_duration - elapsed_time
    filled_length = int(progress_length * elapsed_time // collection_duration)
    bar = '‚îÅ' * filled_length + ' ' * (progress_length - filled_length)

    # Print the progress bar in a single line, updating in place
    sys.stdout.write(f'\r[{bar}] [{elapsed_time}/{collection_duration}sec]')
    sys.stdout.flush()

def plot_all_data_in_one_windows():
    # Plotting all metrics in a single figure
    plt.figure(figsize=(16, 10))  # Set a larger figure size for better visibility

    # Loop through each metric to plot it on the same figure
    for metric in metrics:
        metric_id = metric['id']
        plt.plot(df['Timestamp'], df[metric_id], marker='o', label=metric_id)

    # Configure the common plot settings
    plt.title("Metrics over Time")
    plt.xlabel('Time')
    plt.xticks(rotation=45)
    plt.legend(loc='best')  # Add legend for better clarity
    plt.tight_layout()
    plt.grid(True)

    # Save the combined plot to the specified directory
    plot_filename = os.path.join(output_dir, "combined_metrics_plot.png")
    plt.savefig(plot_filename)
    plt.show()  # Display the combined plot
    print(f"Combined plot saved to '{plot_filename}'.")

def plot_all_data_in_sub_window():
    # Generate the subplots
    num_metrics = len(metrics)
    rows, cols = 4, 2  # Set up a 3x2 grid for subplots
    fig, axs = plt.subplots(rows, cols, figsize=(15, 10))  # Create a figure with subplots
    fig.suptitle("Metrics Over Time", fontsize=16)

    # Plot each metric in a separate subplot
    print("Generating plots...")
    for idx, metric in enumerate(metrics):
        metric_id = metric['id']
        row = idx // cols
        col = idx % cols
        ax = axs[row, col]  # Get the correct subplot

        # Plot the metric on its subplot
        ax.plot(df['Timestamp'], df[metric_id], marker='o')
        ax.set_title(f"{metric_id} over Time")
        ax.set_xlabel('Time')
        if metric_id == 'memory.limit.used.percent':
            ax.set_ylabel('Memory Limit Used (%)')
        elif metric_id == 'net.request.time':
            ax.set_ylabel('Net Request Time (ms)')
        elif metric_id == 'cpu.quota.used.percent':
            ax.set_ylabel('CPU Quota Used (%)')
        elif metric_id == 'net.http.error.count':
            ax.set_ylabel('HTTP Error Count')
        elif metric_id == 'cpu.used.percent':
            ax.set_ylabel('CPU Used (%)')
        else:
            ax.set_ylabel(metric_id)
        ax.grid(True)
        ax.tick_params(axis='x', rotation=45)  # Rotate x-axis labels for readability

    # Hide any unused subplots
    for idx in range(num_metrics, rows * cols):
        fig.delaxes(axs.flatten()[idx])

    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to fit title
    plot_filename = os.path.join(output_dir, "combined_metrics_plot_subs.png")
    plt.savefig(plot_filename)
    plt.show()

######################  Continuous monitoring loop ##########################
print("##1## Starting data collection...")
while time.time() < collection_end_time:
    ok, res = sdclient.get_data(metrics, start, end, sampling, filter=filter)
    if ok:
        # Collect data
        for d in res['data']:
            # Convert timestamp
            data_time = d['t'] / 1000 if d['t'] > 1e10 else d['t']
            readable_time = datetime.fromtimestamp(data_time)

            # Extract metric values
            data_row = {'Timestamp': readable_time}
            for i, metric in enumerate(metrics):
                value = d['d'][i]

                # Convert units if necessary
                if metric['id'] == 'net.http.request.time' or metric['id'] == 'net.http.request.time.worst':
                    value = float(value) / 1e6  # Convert ns to ms   # Assuming the value is in nanoseconds, convert to milliseconds
                else:
                    value = float(value)

                data_row[metric['id']] = value

            # Append to the data list
            data_list.append(data_row)
    else:
        print(f"Failed to fetch data: {res}", flush=True)
        sys.exit(1)
    ## Display progress bar
    display_progress_bar()

    time.sleep(sampling)  # wait for the next sampling interval

print("##2## Data collection complete.")

# Create DataFrame from the collected data
df = pd.DataFrame(data_list)

# Remove duplicate entries based on the 'Timestamp' column
df.drop_duplicates(subset=['Timestamp'], inplace=True)

# Save the collected data to a CSV file
os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist
df.to_csv(os.path.join(output_dir, "metrics_dataset.csv"), index=False)
print("##3## Dataset saved to 'metrics_dataset.csv'.")

# Plotting each metric
print("##4## Generating plots...")
# plot_all_data_in_one_windows()
plot_all_data_in_sub_window()

print("#### All done.")
