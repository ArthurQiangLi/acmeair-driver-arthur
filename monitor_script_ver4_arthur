#!/usr/bin/env python

import sys, os, requests
import time
from datetime import datetime, timedelta
import pandas as pd
import matplotlib.pyplot as plt
from sdcclient import SdMonitorClient, IbmAuthHelper
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning) # to suppress the InsecureRequestWarning Warning from http requests.

# Configuration values
GUID = '3fed93bc-00f4-4651-8ce2-e73ba4b9a918'
APIKEY = 'mMdgK_7zUpsjHuRmBuQaut8GqecNMqw76XpWMRoQDSxK'
URL = 'https://ca-tor.monitoring.cloud.ibm.com'

# Setup the Sysdig monitoring client
ibm_headers = IbmAuthHelper.get_headers(URL, APIKEY, GUID)
sdclient = SdMonitorClient(sdc_url=URL, custom_headers=ibm_headers)

# Define metrics to monitor
metrics = [
    # {"id": "net.http.error.count", "aggregations": {"time": "avg", "group": "avg"}},
    # {"id": "memory.limit.used.percent", "aggregations": {"time": "avg", "group": "avg"}},
    # {"id": "net.http.ttfb.avg", "aggregations": {"time": "avg", "group": "avg"}}, 
    # {"id": "net.http.requests.count", "aggregations": {"time": "rate", "group": "avg"}}, 
    {"id": "cpu.quota.used.percent", "aggregations": {"time": "avg", "group": "avg"}},
    {"id": "cpu.used.percent", "aggregations": {"time": "avg", "group": "avg"}},
    {"id": "net.request.time", "aggregations": {"time": "max", "group": "avg"}},
    {"id": "net.http.request.time", "aggregations": {"time": "avg", "group": "avg"}}, 
    {"id": "net.http.request.time.worst", "aggregations": {"time": "avg", "group": "max"}}, 
]

# METRICS = [ 
#            {"id": "http.error.count", "aggregations": {"time": "sum", "group": "sum"}}, 
#            {"id": "container.memory.usage.percent", "aggregations": {"time": "avg", "group": "avg"}} ]

# Namespace filter
filter = "kubernetes.namespace.name = 'group-7'"

############################### Parameters #################################
ENABLE_SELF_ADAPTATION = True#True
sampling = 10  # Sampling time in seconds
collection_duration = 60*10  # in seconds, from now to N seconds later

# Utility Function Weights
WEIGHTS = {
    "average_response_time": 0.4,
    "max_response_time": 0.4,
    "cpu_utilization": 0.2
}

# Quality Requirements Thresholds
THRESHOLDS = {
    "average_response_time": {
        "excellent": 150,
        "acceptable": 300
    },
    "max_response_time": {
        "excellent": 250,
        "acceptable": 500
    },
    "cpu_utilization": {
        "low": 10,
        "high": 20
    }
}

UTILITY_THRESHOLDS = {
    "Optimal": 1.0,
    "Acceptable": 0.4,
    "Critical": 0.2
}

progress_length = 60  # Length of the progress bar
output_dir = "./monitor_collections/" # Specify the output directory for saving plots

# OpenShift parameters
openshift_server = "https://c104-e.ca-tor.containers.cloud.ibm.com:30227"
namespace = "group-7"
# deployment_name = "acmeair-mainservice"
token = "sha256~5sZfQ6BaI1YPz4XqzjKgC04i01Vb-P9_vF7Z850xtGE"#"sha256~aUU7OCqVDPkmNTE30k6urvTjg-Pw5Y6OaLbP0xzpWr8"

# Headers for OpenShift API
headers = {
    "Authorization": f"Bearer {token}",
    "Accept": "application/json",
    "Content-Type": "application/json"
}

# Define the OpenShift API URL for scaling
# scale_url = f"{openshift_server}/apis/apps/v1/namespaces/{namespace}/deployments/{deployment_name}/scale"

deployments = {
    # "acmeair-mainservice": f"{openshift_server}/apis/apps/v1/namespaces/{namespace}/deployments/acmeair-mainservice/scale",
    "acmeair-authservice": f"{openshift_server}/apis/apps/v1/namespaces/{namespace}/deployments/acmeair-authservice/scale",
    "acmeair-bookingservice": f"{openshift_server}/apis/apps/v1/namespaces/{namespace}/deployments/acmeair-bookingservice/scale",
    # "acmeair-customerservice": f"{openshift_server}/apis/apps/v1/namespaces/{namespace}/deployments/acmeair-customerservice/scale",
    "acmeair-flightservice": f"{openshift_server}/apis/apps/v1/namespaces/{namespace}/deployments/acmeair-flightservice/scale"
    # "acmeair-booking-db": f"{openshift_server}/apis/apps/v1/namespaces/{namespace}/deployments/acmeair-booking-db/scale",
    # "acmeair-customer-db": f"{openshift_server}/apis/apps/v1/namespaces/{namespace}/deployments/acmeair-customer-db/scale",
    # "acmeair-flight-db": f"{openshift_server}/apis/apps/v1/namespaces/{namespace}/deployments/acmeair-flight-db/scale"
}

############################# Initiating data ##############################
# Define the time window for fetching data
start = -sampling  # Fetch data for the last 'sampling' seconds
end = 0

data_list = []# Initialize a list to store collected data
collection_end_time = time.time() + collection_duration# Define data collection duration
############################### Functions ##################################

def display_progress_bar():
    elapsed_time = int(time.time() - (collection_end_time - collection_duration))
    remaining_time = collection_duration - elapsed_time
    filled_length = int(progress_length * elapsed_time // collection_duration)
    bar = '‚îÅ' * filled_length + ' ' * (progress_length - filled_length)

    # Print the progress bar in a single line, updating in place
    sys.stdout.write(f'\r[{bar}] [{elapsed_time}/{collection_duration}sec]')
    sys.stdout.flush()

def plot_all_data_in_one_windows():
    # Plotting all metrics in a single figure
    plt.figure(figsize=(16, 10))  # Set a larger figure size for better visibility

    # Loop through each metric to plot it on the same figure
    for metric in metrics:
        metric_id = metric['id']
        plt.plot(df['Timestamp'], df[metric_id], marker='o', label=metric_id)

    # Configure the common plot settings
    plt.title("Metrics over Time")
    plt.xlabel('Time')
    plt.xticks(rotation=45)
    plt.legend(loc='best')  # Add legend for better clarity
    plt.tight_layout()
    plt.grid(True)

    # Save the combined plot to the specified directory
    plot_filename = os.path.join(output_dir, "combined_metrics_plot.png")
    plt.savefig(plot_filename)
    plt.show()  # Display the combined plot
    print(f"Combined plot saved to '{plot_filename}'.")

def plot_all_data_in_sub_window():
    # Generate the subplots
    num_metrics = len(metrics) + 1  # <-- Include 'total_utility' as an additional metric
    rows, cols = 4, 2  # Set up a 3x2 grid for subplots
    fig, axs = plt.subplots(rows, cols, figsize=(15, 10))  # Create a figure with subplots
    fig.suptitle("Metrics Over Time", fontsize=16)

    # Plot each metric in a separate subplot, including 'total_utility'
    print("Generating plots...")
    for idx, metric in enumerate(metrics + [{"id": "total_utility"}]):  # <-- Added 'total_utility'
    #for idx, metric in enumerate(metrics):
        metric_id = metric['id']
        row = idx // cols
        col = idx % cols
        ax = axs[row, col]  # Get the correct subplot

        # Plot the metric on its subplot, if the data is present in the DataFrame
        if metric_id in df.columns:
            ax.plot(df['Timestamp'], df[metric_id], marker='o')
            ax.set_title(f"{metric_id}")
            ax.set_xlabel('Time')
            
            # Assign y-axis labels based on the metric ID
            if metric_id == 'memory.limit.used.percent':
                ax.set_ylabel('Memory Limit Used (%)')
            elif metric_id == 'net.http.request.time':
                ax.set_ylabel('Net Request Time (ms)')
                ax.set_ylim(0, 600)
            elif metric_id == 'cpu.quota.used.percent':
                ax.set_ylabel('CPU Quota Used (%)')
            elif metric_id == 'net.http.request.time.worst':
                ax.set_ylabel('Net Request Time Worst(ms)')
                ax.set_ylim(0, 600)
            elif metric_id == 'total_utility':
                ax.set_ylabel('Total Utility')
                ax.set_ylim(0, 1.1)  # Set y-axis range for utility score
            elif metric_id == 'cpu.used.percent':
                ax.set_ylabel('CPU Used (%)')
            else:
                ax.set_ylabel(metric_id)
            ax.grid(True)
            ax.tick_params(axis='x', rotation=45)  # Rotate x-axis labels for readability

    # Hide any unused subplots
    for idx in range(num_metrics, rows * cols):
        fig.delaxes(axs.flatten()[idx])

    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to fit title
    plot_filename = os.path.join(output_dir, "combined_metrics_plot_subs.png")
    plt.savefig(plot_filename)
    plt.show()

def calculate_utility(value, thresholds, is_cpu=False):
    """
    Calculate the utility score based on the value and defined thresholds.
    Args:
        value (float): The metric value.
        thresholds (dict): Thresholds for utility calculation.
        is_cpu (bool): Flag indicating if the metric is CPU utilization.

    Returns:
        float: Utility score (0, 0.5, or 1).
    """
    if not is_cpu:
        if value <= thresholds["excellent"]:
            return 1.0
        elif thresholds["excellent"] < value <= thresholds["acceptable"]:
            return 0.5
        else:
            return 0.0
    else:
        if value < thresholds["low"]:
            return 1.0
        elif thresholds["low"] <= value <= thresholds["high"]:
            return 0.5
        else:
            return 0.0

def realtime_analyzing():
    """
    Print real-time analysis for specific metrics.
    """
    # Check if there is data collected in the current iteration
    if not data_list:
        print("### Warning: No data available in data_list.")
        return 0  # Return a default utility score if no data is available

    # Get the latest data row
    latest_data = data_list[-1]  # Access the last row of the collected data
    
    # Safely retrieve metric values with default fallbacks
    net_request_time = latest_data.get('net.http.request.time', None)
    net_request_time_worst = latest_data.get('net.http.request.time.worst', None)
    cpu_quota_used = latest_data.get('cpu.quota.used.percent', None)
    
    # Check if required metrics are available
    if net_request_time is None or net_request_time_worst is None or cpu_quota_used is None:
        print("Warning: Some required metrics are missing in the latest data.")
        return 0  # Return a default utility score if critical metrics are missing
    
    # Calculate individual utilities
    u_avg_response = calculate_utility(net_request_time, THRESHOLDS["average_response_time"])
    u_max_response = calculate_utility(net_request_time_worst, THRESHOLDS["max_response_time"])
    u_cpu = calculate_utility(cpu_quota_used, THRESHOLDS["cpu_utilization"], is_cpu=True)
    
    # Calculate total utility
    total_utility = (
        WEIGHTS["average_response_time"] * u_avg_response +
        WEIGHTS["max_response_time"] * u_max_response +
        WEIGHTS["cpu_utilization"] * u_cpu
    )
    
    # Determine overall status
    if total_utility == UTILITY_THRESHOLDS["Optimal"]:
        status = "Optimal"
    elif UTILITY_THRESHOLDS["Acceptable"] <= total_utility < UTILITY_THRESHOLDS["Optimal"]:
        status = "Acceptable"
    else:
        status = "Critical"
    
    print(f"Utility={total_utility:.1f}, Status={status}, avg={net_request_time:.0f}ms, max={net_request_time_worst:.0f}ms, cpu={cpu_quota_used:.0f}%.")
    latest_data['total_utility'] = total_utility  # <-- Store total_utility as a new metric
    return total_utility


def planning(total_utility, current_pods): # return the pod planned.
    if total_utility <= 0.6:
        current_pods += 1
    elif total_utility > 0.9:
        current_pods -= 1
    else:
        #print(f"[Planning] Total utility {total_utility:.2f} is sufficient. No action required.")
        return current_pods
    current_pods = max(1, min(current_pods, 3))
    #print(f"[Planning] Total utility is {total_utility:.2f}. Planning pod= {current_pods}.")
    return current_pods

def get_current_replicas(deployment_name, max_retries=5):  # Try N times to get access to OpenShift
    scale_url = deployments[deployment_name]
    for attempt in range(max_retries):
        try:
            get_response = requests.get(scale_url, headers=headers, verify=False)
            if get_response.status_code == 200:
                return get_response.json()["spec"]["replicas"]
            else:
                print(f"Attempt {attempt + 1}: Failed to get replicas for {deployment_name}, error = {get_response.status_code}.")
        except requests.ConnectionError as e:
            print(f"Attempt {attempt + 1}: Connection error for {deployment_name}: {str(e)}.")
        
        time.sleep(2)  # Wait before retrying

    print(f"Max retries exceeded while trying to get replicas for {deployment_name}.")
    return -1  # Indicate failure



def executing(deployment_name, new_replicas, max_retries=5):
    # Step 1: Read the current number of replicas for the specified deployment
    current_replicas = get_current_replicas(deployment_name)
    if current_replicas == -1:
        return       
    if current_replicas == new_replicas:
        #print(f"Current replicas for {deployment_name}: {current_replicas}, no need to change.")
        return
    
    print(f"Executing adaptation plan: Scaling {deployment_name} pods from {current_replicas} to {new_replicas}...")
    scale_payload = {
        "apiVersion": "autoscaling/v1",
        "kind": "Scale",
        "metadata": {"name": deployment_name, "namespace": namespace},
        "spec": {"replicas": new_replicas}
    }

    # Send the PUT request to update the number of replicas
    scale_url = deployments[deployment_name]
    for attempt in range(max_retries):
        try:
            response = requests.put(scale_url, headers=headers, json=scale_payload, verify=False)
            if response.status_code == 200:
                print(f"Successfully scaled {deployment_name} to {new_replicas} replicas.")
                return
            else:
                print(f"Attempt {attempt + 1}: Failed to update replicas for {deployment_name}: {response.status_code} - {response.text}.")
        except requests.ConnectionError as e:
            print(f"Attempt {attempt + 1}: Connection error for {deployment_name}: {str(e)}.")

        time.sleep(2)  # Wait before retrying

    print(f"Max retries exceeded while trying to update replicas for {deployment_name}.")

def get_current_image(deployment_name):
    resource_url = f"{openshift_server}/apis/apps/v1/namespaces/{namespace}/deployments/{deployment_name}"
    try:
        response = requests.get(resource_url, headers=headers, verify=False)
        if response.status_code == 200:
            return response.json()["spec"]["template"]["spec"]["containers"][0]["image"]
        else:
            print(f"Failed to retrieve the current image for {deployment_name}: {response.status_code} - {response.text}")
            return None
    except requests.ConnectionError as e:
        print(f"Connection error while retrieving the current image for {deployment_name}: {str(e)}")
        return None

def adjust_resources(deployment_name, cpu_request, memory_request, cpu_limit, memory_limit, max_retries=5):
    resource_url = f"{openshift_server}/apis/apps/v1/namespaces/{namespace}/deployments/{deployment_name}"
    current_image = get_current_image(deployment_name)
    
    if not current_image:
        print(f"Cannot adjust resources for {deployment_name} because the image could not be retrieved.")
        return

    payload = {
        "spec": {
            "template": {
                "spec": {
                    "containers": [
                        {
                            "name": deployment_name,
                            "image": current_image,  # Include the current image here
                            "resources": {
                                "requests": {
                                    "cpu": cpu_request,
                                    "memory": memory_request
                                },
                                "limits": {
                                    "cpu": cpu_limit,
                                    "memory": memory_limit
                                }
                            }
                        }
                    ]
                }
            }
        }
    }

    # Update headers for a Kubernetes PATCH request
    patch_headers = headers.copy()
    patch_headers["Content-Type"] = "application/merge-patch+json"
    
    for attempt in range(max_retries):
        try:
            response = requests.patch(resource_url, headers=patch_headers, json=payload, verify=False)
            if response.status_code == 200:
                print(f"Successfully adjusted resources for {deployment_name} to CPU request: {cpu_request}, Memory request: {memory_request}")
                return
            else:
                print(f"Attempt {attempt + 1}: Failed to adjust resources for {deployment_name}: {response.status_code} - {response.text}")
        except requests.ConnectionError as e:
            print(f"Attempt {attempt + 1}: Connection error for {deployment_name}: {str(e)}")

        time.sleep(2)  # Wait before retrying

    print(f"Max retries exceeded while trying to adjust resources for {deployment_name}.")



def planning_resource_adjustment(total_utility):
    if total_utility <= 0.6:  # Threshold for critical condition
        # Example values, adjust as needed based on actual usage patterns
        new_cpu_request = "500m"  # 0.5 vCPU
        new_memory_request = "512Mi"
        new_cpu_limit = "1000m"  # 1 vCPU
        new_memory_limit = "1024Mi"

        # Apply resource changes to critical services
        for deployment_name in deployments.keys():
            adjust_resources(deployment_name, new_cpu_request, new_memory_request, new_cpu_limit, new_memory_limit)
    elif total_utility >= 1.0:  # Threshold for optimal condition
        # Scale down the resources if utility is high and resources are under-utilized
        new_cpu_request = "250m"  # 0.25 vCPU
        new_memory_request = "256Mi"
        new_cpu_limit = "500m"  # 0.5 vCPU
        new_memory_limit = "512Mi"

        for deployment_name in deployments.keys():
            adjust_resources(deployment_name, new_cpu_request, new_memory_request, new_cpu_limit, new_memory_limit)
    else:
        print(f"[Planning] Resources are currently optimal for utility level {total_utility}. No adjustment required.")


######################  Continuous monitoring loop ##########################
print(f"##0## Setting pod back to 1, wait {sampling}s...")
current_pods = 1
for deployment_name in deployments.keys():
    executing(deployment_name, current_pods)
time.sleep(sampling)  # make sure pod is set to 1 every time.

print("##1## Starting data collection...")
while time.time() < collection_end_time:
    ok, res = sdclient.get_data(metrics, start, end, sampling, filter=filter)
    if ok:
        # Collect data
        for d in res['data']:
            # Convert timestamp
            data_time = d['t'] / 1000 if d['t'] > 1e10 else d['t']
            readable_time = datetime.fromtimestamp(data_time)

            # Extract metric values
            data_row = {'Timestamp': readable_time}
            for i, metric in enumerate(metrics):
                value = d['d'][i]

                # Convert units if necessary
                if metric['id'] == 'net.http.request.time' or metric['id'] == 'net.http.request.time.worst':
                    value = float(value) / 1e6  # Convert ns to ms   # Assuming the value is in nanoseconds, convert to milliseconds
                else:
                    value = float(value)

                data_row[metric['id']] = value

            # Append to the data list
            data_list.append(data_row)
    else:
        print(f"Failed to fetch data: {res}", flush=True)
        sys.exit(1)
    ## Display progress bar
    display_progress_bar()
    total_utilitity = realtime_analyzing()
    if(ENABLE_SELF_ADAPTATION): # if enabled a2's planning and executing, run these two functions.
        # current_pods = planning(total_utilitity, current_pods)
        # for deployment_name in deployments.keys():
        #     executing(deployment_name, current_pods)

        planning_resource_adjustment(total_utilitity)

    time.sleep(sampling)  # wait for the next sampling interval

print("##2## Data collection complete.")

# Create DataFrame from the collected data
df = pd.DataFrame(data_list)

# Remove duplicate entries based on the 'Timestamp' column
df.drop_duplicates(subset=['Timestamp'], inplace=True)

# Save the collected data to a CSV file
os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist
df.to_csv(os.path.join(output_dir, "metrics_dataset.csv"), index=False)
print("##3## Dataset saved to 'metrics_dataset.csv'.")

# Plotting each metric
print("##4## Generating plots...")
# plot_all_data_in_one_windows()
plot_all_data_in_sub_window()

print("#### All done.")
