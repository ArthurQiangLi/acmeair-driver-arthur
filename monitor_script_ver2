#!/usr/bin/env python

import sys
import time
from datetime import datetime, timedelta
import pandas as pd
import matplotlib.pyplot as plt
from sdcclient import SdMonitorClient, IbmAuthHelper

# Configuration values
GUID = '3fed93bc-00f4-4651-8ce2-e73ba4b9a918'
APIKEY = 'mMdgK_7zUpsjHuRmBuQaut8GqecNMqw76XpWMRoQDSxK'
URL = 'https://ca-tor.monitoring.cloud.ibm.com'

# Setup the Sysdig monitoring client
ibm_headers = IbmAuthHelper.get_headers(URL, APIKEY, GUID)
sdclient = SdMonitorClient(sdc_url=URL, custom_headers=ibm_headers)

# Define metrics to monitor
metrics = [
    {"id": "memory.limit.used.percent", "aggregations": {"time": "avg", "group": "avg"}},
    {"id": "net.request.time", "aggregations": {"time": "max", "group": "avg"}},
    {"id": "cpu.quota.used.percent", "aggregations": {"time": "avg", "group": "avg"}},
    {"id": "net.http.error.count", "aggregations": {"time": "avg", "group": "avg"}},
    {"id": "cpu.used.percent", "aggregations": {"time": "avg", "group": "avg"}}
]

# Namespace filter
filter = "kubernetes.namespace.name = 'group-7'"

# Sampling time in seconds
sampling = 10  # Adjust as needed

# Define the time window for fetching data
start = -sampling  # Fetch data for the last 'sampling' seconds
end = 0

# Initialize a list to store collected data
data_list = []

# Define data collection duration
collection_duration = 60  # in seconds
collection_end_time = time.time() + collection_duration

print("Starting data collection...")

# Continuous monitoring loop
while time.time() < collection_end_time:
    ok, res = sdclient.get_data(metrics, start, end, sampling, filter=filter)
    if ok:
        # Collect data
        for d in res['data']:
            # Convert timestamp
            data_time = d['t'] / 1000 if d['t'] > 1e10 else d['t']
            readable_time = datetime.fromtimestamp(data_time)

            # Extract metric values
            data_row = {'Timestamp': readable_time}
            for i, metric in enumerate(metrics):
                value = d['d'][i]

                # Convert units if necessary
                if metric['id'] == 'net.request.time':
                    # Assuming the value is in nanoseconds, convert to milliseconds
                    value = float(value) / 1e6  # Convert ns to ms
                else:
                    value = float(value)

                data_row[metric['id']] = value

            # Append to the data list
            data_list.append(data_row)
    else:
        print(f"Failed to fetch data: {res}", flush=True)
        sys.exit(1)

    time.sleep(sampling)  # wait for the next sampling interval

print("Data collection complete.")

# Create DataFrame from the collected data
df = pd.DataFrame(data_list)

# Remove duplicate entries based on the 'Timestamp' column
df.drop_duplicates(subset=['Timestamp'], inplace=True)

# Save the collected data to a CSV file
df.to_csv('metrics_dataset.csv', index=False)
print("Dataset saved to 'metrics_dataset.csv'.")

# Plotting each metric
print("Generating plots...")
for metric in metrics:
    metric_id = metric['id']
    plt.figure(figsize=(12, 6))
    plt.plot(df['Timestamp'], df[metric_id], marker='o')
    plt.title(f"{metric_id} over Time")
    plt.xlabel('Time')
    if metric_id == 'memory.limit.used.percent':
        plt.ylabel('Memory Limit Used (%)')
    elif metric_id == 'net.request.time':
        plt.ylabel('Net Request Time (ms)')
    elif metric_id == 'cpu.quota.used.percent':
        plt.ylabel('CPU Quota Used (%)')
    elif metric_id == 'net.http.error.count':
        plt.ylabel('HTTP Error Count')
    elif metric_id == 'cpu.used.percent':
        plt.ylabel('CPU Used (%)')
    else:
        plt.ylabel(metric_id)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.grid(True)
    plot_filename = f"{metric_id.replace('.', '_')}_plot.png"
    plt.savefig(plot_filename)
    plt.show()
    print(f"Plot saved to '{plot_filename}'.")

print("All plots generated.")
